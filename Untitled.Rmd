---
title: "Star Wars Scripts - Natural Language Processing"
author: "Steven Pawley"
date: "16/08/2021"
output: 
  html_document: 
    theme: yeti
    highlight: zenburn
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# The Data

```{r}
library(here)
library(tidyverse)
library(tidytext)
library(textdata)

theme_set(theme_minimal())
```

```{r read-data}
newhope <-
  read_delim(
    here("data/SW_EpisodeIV.txt"),
    delim = " ",
    trim_ws = TRUE,
    skip = 1,
    col_names = c("line", "actor", "dialogue")
  )

empire <-
  read_delim(
    here("data/SW_EpisodeV.txt"),
    delim = " ",
    trim_ws = TRUE,
    skip = 1,
    col_names = c("line", "actor", "dialogue")
  )

jedi <-
  read_delim(
    here("data/SW_EpisodeVI.txt"),
    delim = " ",
    trim_ws = TRUE,
    skip = 1,
    col_names = c("line", "actor", "dialogue")
  )

sw <- bind_rows(
  `New Hope` = newhope,
  `Empire` = empire,
  `ROTJ` = jedi,
  .id = "movie"
) |> 
  mutate(
    actor = str_to_title(actor),
    movie = factor(movie, levels = c("New Hope", "Empire", "ROTJ"))
  )
```

# Tokenization

```{r tokenize-sentences}
sw <- sw |>
  unnest_sentences("sentences", "dialogue", drop = FALSE) |>
  nest(sentences = sentences)
```

```{r number-of-sentences-per-character}
sw |>
  mutate(movie = factor(movie, levels = c("New Hope", "Empire", "ROTJ"))) |>
  unnest(sentences) |>
  group_by(actor, movie) |>
  count() |>
  filter(n > 10) |>
  ggplot(aes(y = reorder(actor, n), x = n)) +
  geom_col(width = 0.5) +
  facet_wrap(vars(movie)) +
  theme(axis.title = element_blank())
```

```{r tokenize-words}
sw <- sw |>
  unnest(sentences) |>
  unnest_tokens("words", "sentences", drop = FALSE) |>
  nest(sentences = sentences, words = words)
```

```{r length-of-sentences}
# length of sentences per character
sw <- sw |>
  mutate(length = map_dbl(words, nrow))

sw |>
  group_by(movie, actor) |>
  group_modify(~ mutate(.x, n = n()) |> filter(n > 10)) |>
  summarize(length = median(length)) |>
  ggplot(aes(y = actor, x = length)) +
  geom_col() +
  facet_wrap(vars(movie), scales = "free")
```

# TF-IDF

```{r tf-idf}
# tf-idf
sw_words <- sw |>
  unnest(words) |>
  count(movie, words, sort = TRUE)

sw_total <- sw_words |>
  group_by(movie) |>
  summarize(total = n())

sw_words <- left_join(sw_words, sw_total) |>
  group_by(movie) |>
  mutate(tf = n / total, rank = row_number()) |>
  ungroup()

ggplot(sw_words, aes(tf, fill = movie)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(vars(movie), ncol = 2, scales = "free_y")

ggplot(sw_words, aes(rank, tf, colour = movie)) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10() +
  geom_abline(slope = -1, linetype = 3)

sw_tf_idf <- sw_words |>
  bind_tf_idf(words, movie, n)

sw_tf_idf |>
  arrange(desc(tf_idf))

sw_tf_idf %>%
  group_by(movie) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(words, tf_idf), fill = movie)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(vars(movie), ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

# Unigram Analysis

```{r most-common-words}
# most common words by character (excluding stopwords)
sw |>
  mutate(words = map(words, ~ filter(.x, !words %in% stop_words$word))) |>
  unnest(words) |>
  group_by(movie, actor) |>
  count(words) |>
  ungroup() |>
  filter(actor == "Luke", n > 3) |>
  arrange(desc(n)) |>
  ggplot(aes(y = words, x = n, fill = actor)) +
  geom_col() +
  facet_wrap(vars(movie), scales = "free")
```
